# Implement the following functions

# Activation (sigmoid) function
def sigmoid(x):
    """
    Sigmoid activation function
    ğœ(ğ‘¥)=1/(1+ğ‘’âˆ’ğ‘¥)
    """
    sig = 1 / (1 + np.exp(-x))
    return sig

# Output (prediction) formula
def output_formula(features, weights, bias):
    """
    Output (prediction) formula
    y_hat =ğœ(ğ‘¤1ğ‘¥1+ğ‘¤2ğ‘¥2+ğ‘)
    """
    pred = sigmoid(np.dot(features, weights) + bias)
    return pred

# Error (log-loss) formula
def error_formula(y, output):
    """
    Error function
    ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ(ğ‘¦,y_hat )=âˆ’ğ‘¦log(y_hat)âˆ’(1âˆ’ğ‘¦)log(1âˆ’y_hat)
    """
    error = - y*np.log(output) - (1 - y) * np.log(1-output)
    return error

# Gradient descent step
def update_weights(x, y, weights, bias, learnrate):
    """
    The function that updates the weights
    ğ‘¤ğ‘–âŸ¶ğ‘¤ğ‘–+ğ›¼(ğ‘¦âˆ’y_hat)ğ‘¥ğ‘–
    
    ğ‘âŸ¶ğ‘+ğ›¼(ğ‘¦âˆ’y_hat)
    """
    # Use output function to calculate the output
    output = output_formula(x,weights,bias)
    delta_error = y - output
    weights += learnrate * delta_error * x # ğ‘¤ğ‘–âŸ¶ğ‘¤ğ‘–+ğ›¼(ğ‘¦âˆ’y_hat)ğ‘¥ğ‘–
    bias += learnrate * delta_error        # ğ‘âŸ¶ğ‘+ğ›¼(ğ‘¦âˆ’y_hat)
    return weights, bias